---
title: "svm_gibbs_final"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: "2024-04-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval = FALSE}
# don't use
#library(devtools)
#install_github("olafmersmann/truncnorm", force = TRUE)

```

# Import the dataset

```{r}
library(quantmod)

data <- getSymbols('^GSPC', src='yahoo', auto.assign=FALSE)

# Get returns
returns <- diff(log(Cl(data)))
returns <- returns[-1]
returns <- as.vector(returns)

# Store snp data
snp_data <- as.data.frame(data)
time <- row.names(snp_data)

rm(data)

plot.ts(returns, main = "S&P 500 Returns")
```

```{r}
# Store length of returns vector and initial parameter values
T <- length(returns)
initial <- list(h = rep(0,T), sigma2 = 0.02, phi = 0.95, mu = 0)
```

# Preliminaries for Gibbs Sampling Algorithm 

## Defining the hyperparameters

```{r}
# Hyper-parameters for ht sampler 
nu2 <- initial$sigma2 / (1 + initial$phi^2); tau <- initial$sigma2 / (1 - initial$phi^2)

# Hyper-parameters for sigma2 sampler
alpha_sigma <- 2.5; beta_sigma <- 0.025

# Hyper-parameters for phi sampler 
alpha_phi <- 0; beta2_phi <- 1 

# Hyper-parameters for mu sampler 
alpha_mu <- 0; beta2_mu <- 100

# List storing all hyper-parameters
hyperparameters <- list(h = c(nu2, tau), sigma2 = c(alpha_sigma, beta_sigma),
                        phi = c(alpha_phi, beta2_phi), mu = c(alpha_mu, beta2_mu))
```

## Function for sampling ht

```{r}
# Function for sampling ht
sample_ht <- function(data, hyperparameters, h, sigma, phi, mu) {
  
  ## Parameters needed
  T <- length(data)
  
  ## Updated h_1
  h[1] <- rnorm(1, mean = mu, sd = sqrt(hyperparameters[2]))

  ## Update h_[2:T-1]
  for (t in 2:(T-1)) {
    
    ### Parameters for proposal distribution
    hstar <- mu + (phi * (h[t-1] - mu + h[t+1] - mu)) / (1 + phi^2)
    psi <- hstar + (hyperparameters[1] / 2) * (data[t]^2 * exp(-hstar) - 1)

    ### Rejection Sampling 
    repeat{
      proposal <- rnorm(1, psi, hyperparameters[2])
      u <- runif(1, min = 0, max = 1)
      
      #### Acceptance probability
      log_fstar <- (-1 / 2) * proposal - (data[t]^2 / 2) * (exp(-proposal))
      log_gstar <- (-1 / 2) * proposal - (data[t]^2 / 2) * (exp(-hstar) * (1 + hstar) - 
                                                           proposal * exp(-hstar))
      prob <- exp(log_fstar - log_gstar)
      
      #### Decide whether to keep or discard sample
      if (u <= prob) {
        h[t] <- proposal
        break # Exit the repeat loop after a successful sample
      }
    }
  }
  
  ## Return updated h vector
  return(h)
}

# Function for sampling sigma2 
sample_sigma2 <- function(data, hyperparameters, h, phi, mu) {
  
  ## Parameters for inverse gamma distribution
  T <- length(data); ht <- h[-T]; ht_plus1 <- h[-1] 
  shape_sigma2 <- hyperparameters[1] + T/2
  scale_sigma2 <- hyperparameters[2] + (1/2) * (h[1] - mu)^2 * (1 - phi^2) 
                    + (1/2) * sum((ht_plus1 - mu - phi * (ht - mu))^2)
  
  ## Draw sigma2 
  sigma2 <- 1 / rgamma(1, shape = shape_sigma2, rate = scale_sigma2)
  return(sigma2)
}

library(truncnorm)
# Function for sampling phi
sample_phi <- function(data, hyperparameters, h, sigma2, mu) {
  
  ## Parameters for truncated normal distribution 
  T <- length(data); ht <- h[-T]; ht_plus1 <- h[-1] 
  var_phi <- (1 / hyperparameters[2] + (-(h[1] - mu)^2 
                                            + sum((ht - mu)^2)) / sigma2)^-1
  mean_phi <- var_phi * (hyperparameters[1] / hyperparameters[2] 
                         + sum((ht_plus1 - mu) * (ht - mu)) / sigma2)
  
  ## Draw phi  
  phi <- rtruncnorm(1, a=-1, b=1, mean = mean_phi, sd = sqrt(var_phi))
  return(phi)
}

# Function for sampling mu 
sample_mu <- function(data, hyperparameters, h, sigma2, phi) {
  
  ## Parameters for normal distribution
  T <- length(data); ht <- h[-T]; ht_plus1 <- h[-1] 
  var_mu <- (1 / hyperparameters[2] + (1 - phi^2 + (T - 1) * 
                                            (1 - phi)^2) / sigma2)^-1
  mean_mu <- var_mu * (hyperparameters[1] / hyperparameters[2] 
                       + (h[1] * (1 - phi^2) + (1 - phi) * sum(ht_plus1 - phi * ht)) / sigma2)
  
  ## Draw mu 
  mu <- rnorm(1, mean = mean_mu, sd = sqrt(var_mu))
  return(mu)
} 
```

# Gibbs Sampling Algorithm

```{r}
# Gibbs sampling 
gibbs_sampling <- function(data, hyperparameters, initial.values, iterations) {
  
  ## Initialize Parameters
  T <- length(data)
  h <- initial.values$h
  sigma2 <- initial.values$sigma2
  phi <- initial.values$phi
  mu <- initial.values$mu
  hyperparameters_h <- hyperparameters$h
  hyperparameters_sigma2 <- hyperparameters$sigma2
  hyperparameters_phi <- hyperparameters$phi
  hyperparameters_mu <- hyperparameters$mu

  # Initialize the matrix to store sampled values
  # +3 for sigma2, phi, and mu
  sampled_values <- matrix(nrow = iterations, ncol = T + 3)
  colnames(sampled_values) <- c(paste("h", 1:T, sep = ""), "sigma2", "phi", "mu")
  
  ## Gibbs Cycle 
  for (g in 1:iterations) {
    # Step 1: Sample h_t
    h <- sample_ht(data, hyperparameters_h, h, sigma2, phi, mu)
    
    # Step 2: Sample sigma2
    sigma2 <- sample_sigma2(data, hyperparameters_sigma2, h, phi, mu)
    
    # Step 3: Sample phi
    phi <- sample_phi(data, hyperparameters_phi, h, sigma2, mu)
    
    # Step 4: Sample mu
    mu <- sample_mu(data, hyperparameters_mu, h, sigma2, phi)
    
    # Save sampled values in the matrix
    sampled_values[g, 1:T] <- h
    sampled_values[g, (T+1):(T+3)] <- c(sigma2, phi, mu)
  }
  
  return(sampled_values)
}
```

# (FINAL) Implementation of Gibbs Sampler on SP500 Data

```{r}
set.seed(123)

# Lists needed as Gibbs input
T <- length(returns)
initial <- list(h = rep(0,T), sigma2 = 0.02, phi = 0.95, mu = 0)
iterations <- 12000

# Gibbs sampling
gibbs_sample_full <- gibbs_sampling(returns, hyperparameters, initial, iterations)
```


# Analysis 

## Data Cleaning & Transformations

NOTE: RUN THIS SECTION ONLY ONCE AS IT IS REMOVING COLUMNS... OTHERWISE RESTART THE GIBBS SAMPLER AND RUN AGAIN)

First we make the full gibbs sample easy to work with.
```{r}
# Convert Gibbs sample matrix into dataframe for easier access
gibbs_sample_full <- as.data.frame(gibbs_sample_full)

# Remove the last h column (not updated)
gibbs_sample_full <- gibbs_sample_full[-(ncol(gibbs_sample_full)-3)]

# Label rows
rownames(gibbs_sample_full) <- paste("Cycle ", 1:(nrow(gibbs_sample_full)))

# Add beta to the sample dataset
gibbs_sample_full$beta <- with(gibbs_sample_full, exp(gibbs_sample_full$mu / 2))

# Export as csv 
#write.csv(gibbs_sample_full, "gibbs-full-sample.csv")
```

Now we discard the burn-in period.
```{r}
# Remove Burn-In Period 
gibbs_sample <- gibbs_sample_full[2001:12000, ]

# View first 6 values of model parameters (use ncol instead of hard-coding b/c data updates every day)
head(gibbs_sample[,c((ncol(gibbs_sample) - 3):(ncol(gibbs_sample)))])
```

## Trace Plots 

First without burn in:
```{r}
par(mfrow = c(1, 3))

# Trace plots of parameters
plot(1:(nrow(gibbs_sample_full)), gibbs_sample_full$sigma2, col = 'firebrick', type = "b", 
     main = "", xlab = "Cycle", ylab = "σ^2 | y", pch = '.')

plot(1:(nrow(gibbs_sample_full)), gibbs_sample_full$phi, col = 'firebrick', type = "b", 
     main = "", xlab = "Cycle", ylab = "ɸ | y", pch = '.')

plot(1:(nrow(gibbs_sample_full)), gibbs_sample_full$mu, col = 'firebrick', type = "b", 
     main = "", xlab = "Cycle", ylab = "μ | y", pch = '.')

#plot(1:(nrow(gibbs_sample)), gibbs_sample_full$beta, col = 'firebrick', type = "b", 
#     main = "", xlab = "Cycle", ylab = "β | y", pch = '.')

mtext("", side = 3, outer = TRUE, padj = 3)

# Reset to default margins
par(mfrow = c(1, 1))  # the default mar values
```

Next with burn in:

```{r}
# Set up the plotting area with adjusted margins
# mar sets the margins as (bottom, left, top, right) in lines
# oma sets the outer margins in lines (bottom, left, top, right)
par(mfrow = c(1, 3))

# Trace plots of parameters
plot(1:(nrow(gibbs_sample)), gibbs_sample$sigma2, col = 'firebrick', type = "b", 
     main = "", xlab = "Cycle", ylab = "σ^2 | y", pch = '.')

plot(1:(nrow(gibbs_sample)), gibbs_sample$phi, col = 'firebrick', type = "b", 
     main = "", xlab = "Cycle", ylab = "ɸ | y", pch = '.')

plot(1:(nrow(gibbs_sample)), gibbs_sample$mu, col = 'firebrick', type = "b", 
     main = "", xlab = "Cycle", ylab = "μ | y", pch = '.')

#plot(1:(nrow(gibbs_sample)), gibbs_sample$beta, col = 'firebrick', type = "b", 
#     main = "", xlab = "Cycle", ylab = "β | y", pch = '.')

mtext("", side = 3, outer = TRUE, padj = 3)

# Reset to default margins
par(mfrow = c(1, 1))  # the default mar values
```

## Summary Statistics and Densities 

```{r}
# Means 
means <- list(sigma2 = mean(gibbs_sample$sigma2), phi = mean(gibbs_sample$phi), mu = mean(gibbs_sample$mu), beta = mean(gibbs_sample$beta))

c(means$sigma2, means$phi, means$mu, means$beta)

# Variance
var <- list(sigma2 = var(gibbs_sample$sigma2), phi = var(gibbs_sample$phi), mu = var(gibbs_sample$mu), beta = var(gibbs_sample$beta))

c(var$sigma2, var$phi, var$mu, var$beta)
```

```{r}
# Set up the plotting area
par(mfrow = c(1, 3))

# Densities of parameters
hist(gibbs_sample$sigma2, freq = FALSE, xlim = c(0, 4e-05), ylim = c(0, 2e+05), 
     col = 'indianred', main = "", xlab = "σ^2 | y")

hist(gibbs_sample$phi, freq = FALSE, xlim = c(0.96, 1), ylim = c(0, 300), 
     col = 'indianred', main = "", xlab = "ɸ | y")

hist(gibbs_sample$mu, freq = FALSE, xlim = c(-9.5,-8.5), ylim = c(0, 10),
     col = 'indianred', main = "", xlab = " μ| y")

#hist(gibbs_sample$beta, freq = FALSE, xlim = c(0.01, 0.012), ylim = c(0, 1000),
#     col = 'indianred', main = "", xlab = " β| y")

mtext("", side = 3, outer = TRUE, padj = 3)

# Reset to default
par(mfrow = c(1, 1))
```

## ACF Plots

```{r}
# Set up the plotting area
par(mfrow = c(1, 3))

acf(ts(gibbs_sample$sigma2), lag.max = 80, ylab = "ACF for σ^2", main = "", col = 'firebrick')
acf(ts(gibbs_sample$phi), lag.max = 80, ylab = "ACF for ɸ", main = "", col = 'firebrick')
acf(ts(gibbs_sample$mu), lag.max = 80, ylab = "ACF for μ", main = "", col = 'firebrick')
#acf(ts(gibbs_sample$beta), lag.max = 40, ylab = "ACF for β", main = "", col = 'firebrick')

mtext("", side = 3, outer = TRUE, padj = 3)

# Reset to default
par(mfrow = c(1, 1))
```

## Implied Volatility 
NOTE: RUN THIS SECTION ONLY ONCE AS IT IS REMOVING COLUMNS... OTHERWISE RESTART THE GIBBS SAMPLER AND RUN AGAIN)

```{r}
# Calculate the means of each of the first 4355 variables
rm_col <- 20 # set number of ht you want to remove from end b/c of high variance in mean
mean.h <- colMeans(gibbs_sample[, 1:(ncol(gibbs_sample) - (rm_col + 4))], na.rm = TRUE) 
exp.mean.h <- exp(mean.h/2)

# Assuming the row names in 'data' are dates you want to use as labels
x_vals <- time[1:length(exp.mean.h)]

# Creating a numeric sequence for plotting
index <- 1:length(x_vals)

# Plotting using numeric indices but labeling with time values
par(mar=c(5, 4, 4, 2) + 0.1, mgp=c(2, 0.5, 0))
plot(index, exp.mean.h, type = "l", main = "Implied Volatility Over Time",
     xlab = "Time", ylab = "Volatility, exp(h_t/2)", col = "firebrick", lwd = 2, axes = FALSE, xaxt = 'n')

# Add y-axis
axis(2)

# Enhance with grid
grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted")

# Adding a box around the plot
box()

# Selecting exactly four indices for x-axis labels
label_indices <- round(seq(from = 1, to = length(x_vals), length.out = 4))
label_dates <- x_vals[label_indices]

# Adding custom x-axis with four time labels, horizontally aligned
axis(1, at = label_indices, labels = label_dates, las = 1, cex.axis = 0.7)
```

```{r}
# Dataframe for ht (used in LSTM RMD)
mean.h_df <- data.frame(x_vals, mean.h)
#write.csv(mean.h_df, "mean-ht-logscale.csv")
```

## Further Diagnostics: Rhat & SBC

NOTE: These might take a long time to run.

```{r}
#Get samples with initial values pulled from prior distribution
n <- 10 # 100 takes 2 hours, 10 takes 15 mins
#Consider setting the iterations or n to a smaller amount if this takes too long to run
iterations <- 1000
prior_mu <- rnorm(n,0,1)
prior_sigma <- 1/rgamma(n,2.5,0.025)
prior_phi <- 2*rbeta(n,20,1.5) - 1
samples <- array(rep(1, n*iterations*(T+3)), dim=c(n, iterations, T+3))

#Only uses the last iteration so dont need to remove burn-in
```

```{r}
for (i in 1:n){
  initial <- list(h = rep(0,T), sigma2 = prior_sigma[i], phi = prior_phi[i], mu = prior_mu[i])
  nu2 <- initial$sigma2 / (1 + initial$phi^2); tau <- initial$sigma2 / (1 - initial$phi^2)
  gs <- gibbs_sampling(returns, hyperparameters, initial, iterations)
  samples[i,1:iterations,1:(T+3)] <- gs
  
}
```

```{r}
h_samples <- samples[1:n,1:iterations,1:(T-1)]
```

```{R}
#SBC
m <- 10
ranks <- matrix(0,nrow=n,ncol=3)
for(i in 1:n){
  mu <- prior_mu[i]
  sigma <- prior_sigma[i]
  phi <- prior_phi[i]
  h <- h_samples[i,iterations,1:(T-1)]
  initial <- list(h = rep(0,T), sigma2 = prior_sigma[i], phi = prior_phi[i], mu = prior_mu[i])
  nu2 <- initial$sigma2 / (1 + initial$phi^2); tau <- initial$sigma2 / (1 - initial$phi^2)
  alpha_sigma <- 2.5; beta_sigma <- 0.025
  alpha_phi <- 0; beta2_phi <- 1 
  alpha_mu <- 0; beta2_mu <- 100
  hyperparameters <- list(h = c(nu2, tau), sigma2 = c(alpha_sigma, beta_sigma),
                        phi = c(alpha_phi, beta2_phi), mu = c(alpha_mu, beta2_mu))
  hyperparameters_sigma2 <- hyperparameters$sigma2
  hyperparameters_phi <- hyperparameters$phi
  hyperparameters_mu <- hyperparameters$mu
  for (j in 1:m){
    sigma2_r <- sample_sigma2(data, hyperparameters_sigma2, h, phi, mu)
    if ((sigma2_r) < sigma){
      ranks[i,1] <- ranks[i,1] + 1
    }
    
    # Step 3: Sample phi
    phi_r <- sample_phi(data, hyperparameters_phi, h, sigma2_r, mu)
    if (phi_r < phi){
      ranks[i,2] <- ranks[i,2] + 1
    }
    
    # Step 4: Sample mu
    mu_r <- sample_mu(data, hyperparameters_mu, h, sigma, phi)
    if (mu_r < mu){
      ranks[i,3] <- ranks[i,3] + 1
    }
    
  }
  
}
```

```{r}
#Results are mostly single bar implying that values obtained from the algorithm are not calibrated to the priors.

hist(ranks[,1],main = "Sigma Rank Distribution")
hist(ranks[,2],main = "Phi Rank Distribution")
hist(ranks[,3],main = "Mu Rank Distribution")
```

```{r}
#Rhat
#Less than 1.1 implies good result and that the different chains have very similiar within-chain and between chain convergence
N <- length(returns)
M <- nrow(gibbs_sample)
vars <- numeric(M)
means <- numeric(M)




for (i in 1:M){
  vars[i] <- sd(unlist(gibbs_sample[i,]))**2
  means[i] <- mean(unlist(gibbs_sample[i,]))
}
W <- mean(vars*((N-1)/N))
B <- (sd(means)**2) * (N-1)
print("Rhat")
print(((((N-1)/N)*W + (1/N)*B)/W)**(1/2))
```

```{r}
ESS <- numeric(N)
for (i in 1:N){
  h <- gibbs_sample[,i]
  p <- acf(h,lag.max = M,plot=FALSE)$acf
  sum_p <- 0
  for (t in 1:length(p)){
    p[t] <- ((M-t)/M) * p[t]
    if (p[t] < 0){
      break
    }
    sum_p <- sum_p + p[t]
    
  }
  ESS[i] <- M/(2*sum_p + 1)
  
}
print("Average ESS")
print(mean(ESS))
```

## Extras (not included in paper) 

```{r}
sigma2 <- gibbs_sample$sigma2; sigma2.lag <- c(NA, sigma2[-length(sigma2)])
phi <- gibbs_sample$phi; phi.lag <- c(NA, phi[-length(phi)])
mu <- gibbs_sample$mu; mu.lag <- c(NA, mu[-length(mu)])
beta <- gibbs_sample$beta; beta.lag <- c(NA, beta[-length(beta)])

# Set up the plotting area
par(mfrow = c(2, 2))  # 2 rows, 2 columns

plot(sigma2.lag, sigma2, xlab = "sigma2.lag", ylab = "sigma2", col = "indianred", type = 'p', cex = 1)
plot(phi.lag, phi, xlab = "phi.lag", ylab = "phi", col = "indianred", type = 'p', cex = 1)
plot(mu.lag, mu, xlab = "beta.lag", ylab = "mu", col = "indianred", type = 'p', cex = 1)
plot(beta.lag, beta, xlab = "beta.lag", ylab = "beta", col = "indianred", type = 'p', cex = 1)

mtext("Lag Plots", side = 3, outer = TRUE, padj = 3)

# Reset to default
par(mfrow = c(1, 1))

```





## Synthetic Control

```{r}

library(quantmod)
library(Synth)
library(dplyr)
library(reshape2)
library(xts)

# Get S&P 500 data
snp_data <- getSymbols('^GSPC', src='yahoo', auto.assign=FALSE)

# Compute log returns
snp_returns <- diff(log(Cl(snp_data)))
snp_returns <- snp_returns[-1]  # Remove NA
dates <- index(snp_returns)  # Store dates
snp_returns <- as.vector(snp_returns)

# Define event date
event_date <- as.Date("2020-03-15")

# Load additional indices for synthetic control
indices <- c("^DJI", "^IXIC", "^RUT", "^FTSE", "^N225")

# Get market data for control group
market_data <- lapply(indices, function(x) getSymbols(x, src="yahoo", auto.assign=FALSE))
market_returns <- lapply(market_data, function(x) diff(log(Cl(x)))[-1])

# Convert to a merged dataframe
market_returns_df <- do.call(merge, market_returns)
colnames(market_returns_df) <- indices

# Align dates with S&P 500 and remove rows with missing values
market_returns_df <- market_returns_df[index(market_returns_df) %in% dates, ]
market_returns_df <- na.omit(market_returns_df)  # Drop rows with any NA

# Update dates after removing NA rows
dates_filtered <- index(market_returns_df)
snp_filtered <- snp_returns[dates %in% dates_filtered]
dates_filtered <- as.Date(dates_filtered)  # Ensure it's Date class

# Convert market returns to dataframe
market_returns_df$Date <- as.numeric(index(market_returns_df))  # Convert dates to numeric

# Convert S&P 500 returns to dataframe and align dates
snp_df <- data.frame(Date = as.numeric(dates_filtered), SP500 = snp_filtered)

# Merge the S&P 500 data with market data
full_data <- merge(snp_df, market_returns_df, by = "Date", all.x = TRUE)

# Reshape into long format
long_data <- melt(full_data, id.vars = "Date", variable.name = "Unit", value.name = "Returns")

# Remove missing values
long_data <- long_data[complete.cases(long_data), ]

# Filter to keep only complete units (i.e., units observed across all time points)
obs_count <- long_data %>%
  group_by(Unit) %>%
  summarise(n_obs = n())

n_dates <- length(unique(long_data$Date))
complete_units <- obs_count$Unit[obs_count$n_obs == n_dates]
long_data <- long_data[long_data$Unit %in% complete_units, ]

# Reassign numeric identifiers to units
long_data$Unit_ID <- as.numeric(factor(long_data$Unit))

# Define pre-event period
pre_event_data <- long_data[long_data$Date < as.numeric(event_date), ]

# Ensure proper sorting and character unit names
pre_event_data <- pre_event_data[order(pre_event_data$Date, pre_event_data$Unit_ID), ]
pre_event_data$Unit <- as.character(pre_event_data$Unit)

# Prepare data for Synth
dataprep_obj <- dataprep(
  foo = pre_event_data,
  predictors = "Returns",  # Predictors: market index returns
  dependent = "Returns",   # Dependent variable: S&P 500 returns
  unit.variable = "Unit_ID",  # Numeric unit identifier
  time.variable = "Date",  # Numeric time variable
  treatment.identifier = unique(long_data$Unit_ID[long_data$Unit == "SP500"]),  # S&P 500 is treated unit
  controls.identifier = unique(long_data$Unit_ID[long_data$Unit != "SP500"]),  # Other indices as control
  time.predictors.prior = sort(unique(pre_event_data$Date)),  # Pre-treatment period
  time.optimize.ssr = sort(unique(pre_event_data$Date)),  # Optimize over pre-treatment period
  unit.names.variable = "Unit"
)

# Run synthetic control model
synth_res <- synth(dataprep_obj)

# Extract synthetic control estimated returns
synthetic_returns <- dataprep_obj$Y0 %*% synth_res$solution.w

# Convert synthetic returns to time series
synthetic_ts <- xts(synthetic_returns, order.by = dates_filtered[dates_filtered < event_date])

# Plot actual vs synthetic returns
plot(dates_filtered[dates_filtered < event_date], snp_filtered[dates_filtered < event_date], type = "l", 
     col = "red", lwd = 2, main = "Actual vs. Synthetic S&P 500 Returns", 
     ylab = "Returns", xlab = "Time")
lines(index(synthetic_ts), synthetic_ts, col = "blue", lwd = 2)
legend("topright", legend = c("Actual S&P 500", "Synthetic Control"), 
       col = c("red", "blue"), lwd = 2)

# Store synthetic control values for further analysis
synthetic_df <- data.frame(Date = dates_filtered[dates_filtered < event_date], 
                           Actual = snp_filtered[dates_filtered < event_date], 
                           Synthetic = synthetic_returns)



```


## update weights to post

```{r}
library(quantmod)
library(Synth)
library(dplyr)
library(reshape2)
library(xts)

# Set date range and event
start_date <- as.Date("2018-01-01")
end_date <- as.Date("2022-03-15")
event_date <- as.Date("2020-03-16")  # First trading day after COVID-19 crash trigger

# Get S&P 500 data
snp_data <- getSymbols('^GSPC', src='yahoo', auto.assign=FALSE)
snp_data <- snp_data[paste0(start_date, "/", end_date)]
snp_returns <- diff(log(Cl(snp_data)))
snp_returns <- snp_returns[-1]
dates <- index(snp_returns)
snp_returns <- as.vector(snp_returns)

# Load additional indices
indices <- c("^DJI", "^IXIC", "^RUT")
market_data <- lapply(indices, function(x) {
  data <- getSymbols(x, src = "yahoo", auto.assign = FALSE)
  return(data[paste0(start_date, "/", end_date)])
})

market_returns <- lapply(market_data, function(x) diff(log(Cl(x)))[-1])
market_returns_df <- do.call(merge, market_returns)
colnames(market_returns_df) <- indices
market_returns_df <- market_returns_df[index(market_returns_df) %in% dates, ]
market_returns_df <- na.omit(market_returns_df)

dates_filtered <- index(market_returns_df)
snp_filtered <- snp_returns[dates %in% dates_filtered]
dates_filtered <- as.Date(dates_filtered)

# Convert to dataframes
market_returns_df$Date <- as.numeric(index(market_returns_df))
snp_df <- data.frame(Date = as.numeric(dates_filtered), SP500 = snp_filtered)
full_data <- merge(snp_df, market_returns_df, by = "Date", all.x = TRUE)

# Reshape to long format
long_data <- melt(full_data, id.vars = "Date", variable.name = "Unit", value.name = "Returns")
long_data <- long_data[complete.cases(long_data), ]

# Balance panel
obs_count <- long_data %>% group_by(Unit) %>% summarise(n_obs = n())
n_dates <- length(unique(long_data$Date))
complete_units <- obs_count$Unit[obs_count$n_obs == n_dates]
long_data <- long_data[long_data$Unit %in% complete_units, ]

# Assign numeric unit IDs
long_data$Unit_ID <- as.numeric(factor(long_data$Unit))

# Pre-event data
pre_event_data <- long_data[long_data$Date < as.numeric(event_date), ]
pre_event_data <- pre_event_data[order(pre_event_data$Date, pre_event_data$Unit_ID), ]
pre_event_data$Unit <- as.character(pre_event_data$Unit)

# Prepare Synth object
dataprep_obj <- dataprep(
  foo = pre_event_data,
  predictors = "Returns",
  dependent = "Returns",
  unit.variable = "Unit_ID",
  time.variable = "Date",
  treatment.identifier = unique(long_data$Unit_ID[long_data$Unit == "SP500"]),
  controls.identifier = unique(long_data$Unit_ID[long_data$Unit != "SP500"]),
  time.predictors.prior = sort(unique(pre_event_data$Date)),
  time.optimize.ssr = sort(unique(pre_event_data$Date)),
  unit.names.variable = "Unit"
)

# Fit synthetic control model
synth_res <- synth(dataprep_obj)

# Pre-event actual and synthetic time series
actual_ts <- xts(snp_filtered, order.by = dates_filtered)
synthetic_pre <- dataprep_obj$Y0 %*% synth_res$solution.w
synthetic_ts_pre <- xts(synthetic_pre, order.by = dates_filtered[dates_filtered < event_date])

# Extract pre/post actuals
actual_pre <- actual_ts[dates_filtered < event_date]
actual_post <- actual_ts[dates_filtered >= event_date]

# Post-event: manually compute synthetic returns
weights <- synth_res$solution.w
post_event_data <- long_data[long_data$Date >= as.numeric(event_date), ]
post_event_data$Unit <- as.character(post_event_data$Unit)

# Match control names to weight order
unit_table <- unique(long_data[, c("Unit_ID", "Unit")])
control_names <- unit_table$Unit[unit_table$Unit_ID %in% as.numeric(rownames(weights))]

# Pivot control units to wide format for post-treatment
control_post_wide <- reshape2::dcast(post_event_data, Date ~ Unit, value.var = "Returns")
control_returns <- control_post_wide[, control_names]

# Multiply control returns by weights
synthetic_post <- as.matrix(control_returns) %*% weights
synthetic_ts_post <- xts(synthetic_post, order.by = as.Date(as.numeric(control_post_wide$Date), origin = "1970-01-01"))

# Plot all
plot(index(actual_pre), actual_pre, type = "l", col = "red", lwd = 2,
     ylim = range(c(actual_ts, synthetic_ts_pre, synthetic_ts_post)),
     main = "Actual vs. Synthetic S&P 500 Returns",
     ylab = "Returns", xlab = "Date")

lines(index(synthetic_ts_pre), synthetic_ts_pre, col = "blue", lwd = 2)
lines(index(actual_post), actual_post, col = "red", lwd = 2, lty = 2)
lines(index(synthetic_ts_post), synthetic_ts_post, col = "blue", lwd = 2, lty = 2)
abline(v = event_date, col = "gray", lty = 2)

legend("topright", legend = c("Actual S&P 500 (Pre)", "Synthetic Control (Pre)",
                              "Actual S&P 500 (Post)", "Synthetic Control (Post)"),
       col = c("red", "blue", "red", "blue"), lwd = 2, lty = c(1, 1, 2, 2))

# Save synthetic control results
synthetic_df_pre <- data.frame(
  Date = dates_filtered[dates_filtered < event_date],
  Actual = as.numeric(actual_pre),
  Synthetic = as.numeric(synthetic_ts_pre)
)

synthetic_df_post <- data.frame(
  Date = index(synthetic_ts_post),
  Actual = as.numeric(actual_post),
  Synthetic = as.numeric(synthetic_ts_post)
)

```




















